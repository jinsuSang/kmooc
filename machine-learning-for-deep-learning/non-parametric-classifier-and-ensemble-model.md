# Non-parametric 구분기와  Ensemble 모델

## Non-parametric 구분기와 KNN 모델

### Dicison Theory 필요성과 사용법

|                  | true spam      | true not spam  |
| ---------------- | -------------- | -------------- |
| predict spam     | true positive  | false positive |
| predict not spam | false negative | true negative  |

- False Positiv - 높은 cost

- False Negative - 낮은 cost

  ![image-20210814185112765](C:/Users/sangjs/AppData/Roaming/Typora/typora-user-images/image-20210814185112765.png)

![image-20210814185221044](C:/Users/sangjs/AppData/Roaming/Typora/typora-user-images/image-20210814185221044.png)

### Non-parametric 구분기

- Parametric 모델

  - 파라미터의 개수가 고정된 모델
    - 스플리팅 노드를 나누는 기준, 피처의 타입

  - 스플리팅 노드 개수는 하이퍼 파라미터로 조정
    - 나이브 베이즈 파라미터의 경우 베이즈 룰에서 활용되었던 각각의 확률값

  - 일반적으로 데이터 많을수록 정확도가 상승한다

  - 문제점
    - 고정된 파라미터로는 더 큰 데이터를 처리하기 힘들다



- Non-parametric  모델
  - 파라미터 개수가 데이터 샘플과 거의 동일하거나 그에 비례함
  - 데이터가 많이 주어지므로 모델이 복잡해진다
  - 대표적인 알고리즘은 KNN 알고리즘이다

### K-Nearest Neighbor 구분기

- 단순한 구조

- 모든 트레이닝 샘플들과 현재 새로운 입력된 데이터와 거리를 측정한다

- 새로운 데이터에 가장 가까운 샘플에 해당하는 라벨을 부여한다

- 가정

  - 비슷한 피처를 가진 샘플은 비슷한 라벨을 가질 것이다

    - 단순히 두 피처 간의 거리 차이를 활용한다

    - 값보다는 비례식에 의미가 있는 경우 각도를 값으로 활용하기도 한다

  - k는 의사결정 나무나 나이브 베이즈에서의 하이퍼 파라미터와 같은 역할을 한다
    - 가까운 거리에 해당하는 k개의 샘플을 고르는 것이다

  #### Effect of k in knn

  - 하이퍼 파라미터를 바꾸는 과정으로 지도 학습의 일반화 성능 조정
  - 오버핏 문제를 해결하기 위해 k를 증가시키는 방향으로 진행된다

  - 학습 데이터가 없다

  #### 문제점과 해결방법

  - 트레이닝 샘플이 많아질수록 시간이 증가한다
  - 피처 크기가 커질수록 느려진다
  - Hierarchical Tree - 대표 샘플들과만 거리를 우선 계산하거나 일부분만 추출하여 계산한다
  - 데이터 양에 따라 저장 비용과 메모리 공간이 증가한다

  #### Cuse of Dimensionality

  - 파라미터가 많아진다는 것은 유사도를 측정하기 매우 어렵다는 것이다



## Ensemble 모델

- 지도학습 알고리즘 성능을 향샹시키기 위해 필수적으로 적용되는 매커니즘

- 여러 방법을 활용하여 하이퍼 파라미터를 튜닝하는 방법

  ## Ensemble model

  - 머신러닝 기법들을 함께 사용하는 방법
  - 메타 클래스파이어라고도 한다
    - Averaging - 가장 직관적인 방법
  - 목표 - 각 구분기보다 성능을 더 높이기 위함이다
  - Averaging
  - stacking
  - 구분기가 여러개 있어도 개별적으로 구동 가능

  ## Random Forest 

  - 각각의 의사결정 나무를 서로 독립적으로 학습 후 많은 의사결정 나무에서 얻은 결과들을 averaging해서 최종 결과 출력
  - 장점
    1. 앙상블 모델, averaging과 stacking이 가지는 최대 장점인 동시에 계산 가능하다
    2. 작동속도가 매우 빠르다

  - 의사결정 나무에 랜덤 포레스트 모델을 한 개 사용하면 비슷한 예측결과를 출력한다

  - Bootstrapping, Random Tree 를 활용해 이를 해결한다

    #### Bootstrap sampling

    - 새로운 데이터셋을 만들어내는 과정
    - 학습 데이터셋을 랜덤하게 특정 개수 만큼의 샘플 추출
    - 랜덤하게 추출하는 과정에서 똑같이 샘플이 두 번 추출될 수 있음

    #### Bagging

    - 각 구분기가 서로 다른 데이터셋으로 학습되어 서로 다른 예측값 출력
    - 서로 다른 예측값을 모두 묶어서 하나의 최종 예측값 출력 -> bagging

    #### Random Trees

    - 스플리팅 알고리즘 단순화
    - 피처 타입을 랜덤하게 선택한다
    - 앙상블 모델 내부의 의사결정 나무들이 서로 독립적인 형태로 구동한다

    #### Random forest

    - Bagging + Random trees
    - 서로 다른 bootstrap 데이터 셋을 개별적인 random tree라 학습한다

## Boosting

- 속도 향상을 목표로 한다
- '각 구분기 결과를 기반으로 구분기의 현재 샘플을 추가적으로 테스트할 건지 결정하는 과정이다
- 클래시 파이어가 여러개인 경우 가장 간단하거나 빠른 구분기를 배치하고 다음에 자세하고 느린 구분기를 배치한다
- 가장 적용되기 좋은 환경은 아웃라이어가 많은 환경이다
- 장점 - 자세한 하자 탐지기 머신러닝 기법을 나중에 적용함으로서 계산량이나 속도를 훨씬 빠르게 할 수 있다

